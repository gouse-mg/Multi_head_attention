{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "m=5 #sizeof dataset\n",
    "n=10 # no of embedding dimensions\n",
    "no_of_heads=3\n",
    "X=tf.random.uniform(shape=(m,n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_layer(no_of_heads,X):\n",
    "    m=X.shape[0]\n",
    "    n=X.shape[1]\n",
    "    size_of_head=n//no_of_heads\n",
    "    W_k=tf.random.uniform(shape=(no_of_heads,n,size_of_head))\n",
    "    W_q=tf.random.uniform(shape=(no_of_heads,n,size_of_head))\n",
    "    W_v=tf.random.uniform(shape=(no_of_heads,n,size_of_head))\n",
    "    embeddings_list=[]\n",
    "    for head in range(no_of_heads):\n",
    "        Q=tf.matmul(X,W_q[head])\n",
    "        K=tf.matmul(X,W_k[head])\n",
    "        V=tf.matmul(X,W_v[head])\n",
    "        attention_weights=tf.nn.softmax(tf.matmul(Q,tf.transpose(K)),axis=1)\n",
    "        new_attention_embeddings=tf.matmul(attention_weights,V)\n",
    "        if head!=0:\n",
    "            new_attention_embeddings=tf.concat([prev_attention_embeddings,new_attention_embeddings],axis=1)\n",
    "            prev_attention_embeddings=new_attention_embeddings\n",
    "            \n",
    "        else:\n",
    "            prev_attention_embeddings=new_attention_embeddings\n",
    "    return new_attention_embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 10), dtype=float32, numpy=\n",
       "array([[1.6483406, 2.0022707, 1.6448619, 1.6328374, 1.6913489, 2.0470252,\n",
       "        2.3282907, 2.1221817, 2.1829455, 2.6720285],\n",
       "       [1.6299025, 2.0186145, 1.6374913, 1.6416355, 1.6962517, 2.073124 ,\n",
       "        2.3435652, 2.1052957, 2.1871107, 2.6774254],\n",
       "       [1.6316898, 2.0171988, 1.6381745, 1.6408572, 1.6957976, 2.0733087,\n",
       "        2.3426034, 2.1040332, 2.1861608, 2.6783214],\n",
       "       [1.6460105, 1.9968781, 1.648156 , 1.6310189, 1.692392 , 2.0722723,\n",
       "        2.3445058, 2.1078682, 2.1880236, 2.676729 ],\n",
       "       [1.6553352, 1.9918965, 1.6490432, 1.6276575, 1.6892045, 2.038158 ,\n",
       "        2.3238864, 2.1306129, 2.1815736, 2.672147 ]], dtype=float32)>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_layer(2,X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
